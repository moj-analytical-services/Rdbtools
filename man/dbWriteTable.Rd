% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/db_commands.R
\name{dbWriteTable,MoJAthenaConnection,character,data.frame-method}
\alias{dbWriteTable,MoJAthenaConnection,character,data.frame-method}
\title{dbWriteTable}
\usage{
\S4method{dbWriteTable}{MoJAthenaConnection,character,data.frame}(
  conn,
  name,
  value,
  overwrite = FALSE,
  append = FALSE,
  row.names = NA,
  field.types = NULL,
  partition = NULL,
  s3.location = NULL,
  file.type = c("tsv", "csv", "parquet", "json"),
  compress = FALSE,
  max.batch = Inf,
  ...
)
}
\arguments{
\item{conn}{A DBIConnection object, as returned by \code{connect_athena()}}

\item{name}{A character string specifying a table name. Names will be
automatically quoted so you can use any sequence of characters, not
just any valid bare table name.}

\item{value}{A data.frame to write to the database.}

\item{overwrite}{Allows overwriting the destination table. Cannot be \code{TRUE} if \code{append} is also \code{TRUE}.}

\item{append}{Allow appending to the destination table. Cannot be \code{TRUE} if \code{overwrite} is also \code{TRUE}. Existing Athena DDL file type will be retained
              and used when uploading data to AWS Athena. If parameter \code{file.type} doesn't match AWS Athena DDL file type a warning message will be created
              notifying user and \code{noctua} will use the file type for the Athena DDL. When appending to an Athena DDL that has been created outside of \code{noctua}.
              \code{noctua} can support the following SerDes and Data Formats.
\itemize{
\item{\strong{csv/tsv:} \href{https://docs.aws.amazon.com/athena/latest/ug/lazy-simple-serde.html}{LazySimpleSerDe}}
\item{\strong{parquet:} \href{https://docs.aws.amazon.com/athena/latest/ug/parquet.html}{Parquet SerDe}}
\item{\strong{json:} \href{https://docs.aws.amazon.com/athena/latest/ug/json.html}{JSON SerDe Libraries}}
}}

\item{row.names}{Either \code{TRUE}, \code{FALSE}, \code{NA} or a string.

If \code{TRUE}, always translate row names to a column called "row_names".
If \code{FALSE}, never translate row names. If \code{NA}, translate
rownames only if they're a character vector.

A string is equivalent to \code{TRUE}, but allows you to override the
default name.

For backward compatibility, \code{NULL} is equivalent to \code{FALSE}.}

\item{field.types}{Additional field types used to override derived types.}

\item{partition}{Partition Athena table (needs to be a named list or vector) for example: \code{c(var1 = "2019-20-13")}}

\item{s3.location}{s3 bucket to store Athena table, must be set as a s3 uri for example ("s3://mybucket/data/").
By default, the s3.location is set to s3 staging directory from \code{\linkS4class{AthenaConnection}} object. \strong{Note:}
When creating a table for the first time \code{s3.location} will be formatted from \code{"s3://mybucket/data/"} to the following
syntax \code{"s3://{mybucket/data}/{schema}/{table}/{parition}/"} this is to support tables with the same name but existing in different
schemas. If schema isn't specified in \code{name} parameter then the schema from \code{dbConnect} is used instead.}

\item{file.type}{What file type to store data.frame on s3, noctua currently supports ["tsv", "csv", "parquet", "json"]. Default delimited file type is "tsv", in previous versions
of \code{noctua (=< 1.4.0)} file type "csv" was used as default. The reason for the change is that columns containing \code{Array/JSON} format cannot be written to
Athena due to the separating value ",". This would cause issues with AWS Athena.
\strong{Note:} "parquet" format is supported by the \code{arrow} package and it will need to be installed to utilise the "parquet" format.
"json" format is supported by \code{jsonlite} package and it will need to be installed to utilise the "json" format.}

\item{compress}{\code{FALSE | TRUE} To determine if to compress file.type. If file type is ["csv", "tsv"] then "gzip" compression is used, for file type "parquet"
"snappy" compression is used. Currently \code{noctua} doesn't support compression for "json" file type.}

\item{max.batch}{Split the data frame by max number of rows i.e. 100,000 so that multiple files can be uploaded into AWS S3. By default when compression
is set to \code{TRUE} and file.type is "csv" or "tsv" max.batch will split data.frame into 20 batches. This is to help the
performance of AWS Athena when working with files compressed in "gzip" format. \code{max.batch} will not split the data.frame
when loading file in parquet format. For more information please go to \href{https://github.com/DyfanJones/RAthena/issues/36}{link}}

\item{...}{Other arguments used by individual methods.}
}
\description{
See \code{\link[noctua:AthenaWriteTables]{noctua::dbWriteTable()}}. Note that you must have write permission to the s3 directory where the data is stored.
In general you will not have this permission for the automatically generated directory generated by \code{connect_athena()}
so you must specify an s3 directory where you do have write permission.
You can do this either as an argument to \code{connect_athena} (which will affect all your Athena transactions), or
specifically to the \code{dbWriteTable} call using the \code{s3.location} argument.
This function calls \code{noctua::dbWriteTable()}, after replacing any references to \verb{__temp__}
in the statement with your temporary database in Athena. Your temporary database will be created
if you do not already have one.
}
\examples{
# Either specify the location to dbWriteTable itself
con <- connect_athena()
dbWriteTable(con, "__temp__.table_name", dataframe, s3.location = "s3://bucket_you_have_write_permission/dir")

# Or to the connection object
con <- connect_athena(staging_dir = "s3://bucket_you_have_write_permission/dir")
dbWriteTable(con, "__temp__.table_name", dataframe)
}
